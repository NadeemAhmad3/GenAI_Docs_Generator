import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import os
import random

# =============================
# Load embeddings (with dummy vocab)
# =============================
def load_embeddings():
    base = "/kaggle/input/word2vec-all-codedocscombined"

    code_vecs = np.load(os.path.join(base, "word2vec_code_embeddings.npy"))
    doc_vecs = np.load(os.path.join(base, "word2vec_docs_embeddings.npy"))
    combined_vecs = np.load(os.path.join(base, "word2vec_FINAL_combined_embeddings_50k.npy"))

    # Create dummy vocab tokens
    code_vocab = [f"code_tok{i}" for i in range(code_vecs.shape[0])]
    doc_vocab = [f"doc_tok{i}" for i in range(doc_vecs.shape[0])]
    combined_vocab = [f"comb_tok{i}" for i in range(combined_vecs.shape[0])]

    # Map word â†’ vector
    code_emb = {w: code_vecs[i] for i, w in enumerate(code_vocab)}
    doc_emb = {w: doc_vecs[i] for i, w in enumerate(doc_vocab)}
    combined_emb = {w: combined_vecs[i] for i, w in enumerate(combined_vocab)}

    return {"code": code_emb, "docs": doc_emb, "combined": combined_emb}


# =============================
# 1. Semantic Similarity Eval
# =============================
def eval_similarity(emb, word_pairs):
    sims, human = [], []
    for w1, w2, score in word_pairs:
        if w1 in emb and w2 in emb:
            v1, v2 = emb[w1], emb[w2]
            sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            sims.append(sim)
            human.append(score)
    return spearmanr(human, sims)[0] if sims else None


# =============================
# 2. Code Completion Accuracy
# =============================
def eval_code_completion(emb, code_samples, top_k=5):
    correct, total = 0, 0
    vocab = list(emb.keys())
    vectors = np.array([emb[w] for w in vocab])

    for tokens in code_samples:
        if len(tokens) < 3:
            continue
        idx = random.randint(1, len(tokens)-2)
        target = tokens[idx]
        if target not in emb:
            continue

        # Context = neighbors
        ctx_vecs = [emb[t] for t in (tokens[idx-1], tokens[idx+1]) if t in emb]
        if not ctx_vecs:
            continue
        ctx = np.mean(ctx_vecs, axis=0)

        # Similarity with all vocab
        sims = np.dot(vectors, ctx) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(ctx))
        top_ids = sims.argsort()[::-1][:top_k]
        top_preds = [vocab[i] for i in top_ids]

        if target in top_preds:
            correct += 1
        total += 1

    return correct / total if total > 0 else None


# =============================
# 3. Documentation Relevance
# =============================
def eval_doc_relevance(code_emb, doc_emb, pairs):
    sims = []
    for code_tokens, doc_tokens in pairs:
        c_vecs = [code_emb[t] for t in code_tokens if t in code_emb]
        d_vecs = [doc_emb[t] for t in doc_tokens if t in doc_emb]
        if not c_vecs or not d_vecs:
            continue
        v_code, v_doc = np.mean(c_vecs, axis=0), np.mean(d_vecs, axis=0)
        sim = np.dot(v_code, v_doc) / (np.linalg.norm(v_code) * np.linalg.norm(v_doc))
        sims.append(sim)
    return np.mean(sims) if sims else None


# =============================
# 4. Visualization Analysis
# =============================
def visualize_embeddings(emb, title="Embeddings"):
    words = list(emb.keys())[:50]   # Take first 50 tokens
    vecs = np.array([emb[w] for w in words])

    # PCA
    reduced = PCA(n_components=2).fit_transform(vecs)
    plt.figure(figsize=(10, 6))
    plt.scatter(reduced[:, 0], reduced[:, 1], c='blue')
    for i, w in enumerate(words):
        plt.annotate(w, (reduced[i, 0], reduced[i, 1]))
    plt.title(f"PCA - {title}")
    plt.show()

    # t-SNE
    tsne = TSNE(n_components=2, perplexity=15, random_state=42)
    tsne_res = tsne.fit_transform(vecs)
    plt.figure(figsize=(10, 6))
    plt.scatter(tsne_res[:, 0], tsne_res[:, 1], c='green')
    for i, w in enumerate(words):
        plt.annotate(w, (tsne_res[i, 0], tsne_res[i, 1]))
    plt.title(f"t-SNE - {title}")
    plt.show()

    # Similarity Histogram
    sims = []
    for _ in range(200):
        w1, w2 = random.sample(words, 2)
        v1, v2 = emb[w1], emb[w2]
        sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        sims.append(sim)
    plt.hist(sims, bins=20, color='orange', edgecolor='black')
    plt.title(f"Similarity Distribution - {title}")
    plt.show()


# =============================
# Main Evaluation
# =============================
if __name__ == "__main__":
    embeddings = load_embeddings()

    # Dummy eval sets since real words not in dummy vocab
    word_pairs = [("code_tok1", "code_tok2", 0.9), ("doc_tok1", "doc_tok5", 0.2)]
    code_samples = [["code_tok1", "code_tok2", "code_tok3"], ["code_tok4", "code_tok5", "code_tok6"]]
    code_doc_pairs = [(["code_tok1", "code_tok2"], ["doc_tok1", "doc_tok2"])]

    results = {}
    for name, emb in embeddings.items():
        print(f"\n=== {name.upper()} EMBEDDINGS ===")
        results[name] = {}
        results[name]["similarity_corr"] = eval_similarity(emb, word_pairs)
        results[name]["code_completion_acc"] = eval_code_completion(emb, code_samples)
        if name == "combined":
            results[name]["doc_relevance"] = eval_doc_relevance(
                embeddings["code"], embeddings["docs"], code_doc_pairs
            )
        visualize_embeddings(emb, title=f"{name} embeddings")

    print("\nFinal Results:")
    for name, metrics in results.items():
        print(name, metrics)
