{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- INSTALL A RELIABLE BLEU SCORE LIBRARY ---\n!pip install sacrebleu\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport sacrebleu\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\nimport json\nimport re\nimport time\nfrom typing import List, Dict, Tuple\n\n# --- Configuration & Hyperparameters (QUALITY-FOCUSED) ---\nBPE_FILES_PATH = \"/kaggle/input/temp-bpe\"\nWORD_2_VEC_EMBEDDINGS_PATH = \"/kaggle/input/word2vec-combine-only/word2vec_FINAL_combined_embeddings_50k.npy\"\n\n# Model Hyperparameters\nENCODER_EMBED_DIM = 300\nHIDDEN_DIM = 256\nLSTM_LAYERS = 2\nENCODER_DROPOUT = 0.4\nDECODER_DROPOUT = 0.4\nLEARNING_RATE = 0.001\nBATCH_SIZE = 64\nNUM_EPOCHS = 8\n\n# Data Settings\nMAX_CODE_LEN = 256\nMAX_SUMMARY_LEN = 50\nTRAINING_SUBSET_SIZE = 250000\n\n# --- Device Configuration ---\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# --- BPE Tokenizer Class (No change) ---\nclass BPE_Loader:\n    def __init__(self):\n        self.token_to_id: Dict[str, int] = {}\n        self.id_to_token: Dict[int, str] = {}\n        self.merges: Dict[Tuple[str, str], int] = {}\n\n    def load(self, file_prefix: str):\n        vocab_path = f\"{file_prefix}_vocab.json\"\n        merges_path = f\"{file_prefix}_merges.json\"\n        if not os.path.exists(vocab_path): raise FileNotFoundError(f\"Missing: {vocab_path}\")\n        if not os.path.exists(merges_path): raise FileNotFoundError(f\"Missing: {merges_path}\")\n        with open(vocab_path, 'r', encoding='utf-8') as f: self.token_to_id = json.load(f)\n        self.id_to_token = {i: t for t, i in self.token_to_id.items()}\n        with open(merges_path, 'r', encoding='utf-8') as f:\n            merges_loaded = json.load(f)\n            self.merges = {tuple(k.split(' ')): v for k, v in merges_loaded.items()}\n        print(f\"Loaded BPE tokenizer from '{file_prefix}'. Vocab size: {len(self.token_to_id)}\")\n\n    def decode(self, token_ids: List[int]) -> str:\n        tokens = [self.id_to_token.get(i, '<UNK>') for i in token_ids]\n        return \"\".join(tokens).replace('</w>', ' ').strip()\n\n    def _tokenize_word(self, word: str) -> List[str]:\n        word_tuple = tuple(word) + ('</w>',)\n        while True:\n            pairs = list(zip(word_tuple[:-1], word_tuple[1:]))\n            applicable_merges = {p: self.merges[p] for p in pairs if p in self.merges}\n            if not applicable_merges: break\n            best_pair = min(applicable_merges, key=applicable_merges.get)\n            new_token, i, new_word_tuple = \"\".join(best_pair), 0, []\n            while i < len(word_tuple):\n                if i < len(word_tuple) - 1 and (word_tuple[i], word_tuple[i+1]) == best_pair:\n                    new_word_tuple.append(new_token); i += 2\n                else: new_word_tuple.append(word_tuple[i]); i += 1\n            word_tuple = tuple(new_word_tuple)\n        return list(word_tuple)\n\n    def encode(self, text: str) -> List[int]:\n        if not isinstance(text, str): return []\n        ids = []\n        unk_id = self.token_to_id.get('<UNK>')\n        words = re.findall(r\"\\w+|\\S\", text)\n        for word in words:\n            for token in self._tokenize_word(word):\n                ids.append(self.token_to_id.get(token, unk_id))\n        return ids","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Check for existing df ---\ntry:\n    print(f\"Using existing DataFrame. Shape: {df.shape}\")\nexcept NameError:\n    raise NameError(\"DataFrame 'df' not found. Please run your data loading cell first.\")\n\n# --- Load Tokenizer and Pre-trained Embeddings ---\nprint(\"Loading BPE and Word2Vec...\")\ntokenizer = BPE_Loader()\ntokenizer.load(os.path.join(BPE_FILES_PATH, 'bpe_combined'))\npretrained_embeddings = np.load(WORD_2_VEC_EMBEDDINGS_PATH)\npretrained_embeddings = torch.tensor(pretrained_embeddings, dtype=torch.float32)\nprint(\"Word2Vec embeddings loaded.\")\n\n# --- Define Special Token IDs ---\nPAD_TOKEN, SOS_TOKEN, EOS_TOKEN = '<PAD>', '<SOS>', '<EOS>'\nPAD_ID, SOS_ID, EOS_ID = tokenizer.token_to_id[PAD_TOKEN], tokenizer.token_to_id[SOS_TOKEN], tokenizer.token_to_id[EOS_TOKEN]\nVOCAB_SIZE = len(tokenizer.token_to_id)\n\n# --- PyTorch Dataset Class ---\nclass CodeSummaryDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_code_len, max_summary_len):\n        self.df = dataframe.dropna(subset=['code', 'summary']).reset_index(drop=True)\n        self.tokenizer, self.max_code_len, self.max_summary_len = tokenizer, max_code_len, max_summary_len\n        self.codes, self.summaries = self.df['code'].values, self.df['summary'].values\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        code_tokens = self.tokenizer.encode(str(self.codes[idx]))[:self.max_code_len]\n        summary_tokens = self.tokenizer.encode(str(self.summaries[idx]))[:self.max_summary_len - 2]\n        code_tensor = torch.tensor(code_tokens + [PAD_ID] * (self.max_code_len - len(code_tokens)), dtype=torch.long)\n        summary_tensor = torch.tensor([SOS_ID] + summary_tokens + [EOS_ID] + [PAD_ID] * (self.max_summary_len - len(summary_tokens) - 2), dtype=torch.long)\n        return code_tensor, summary_tensor\n\n# --- Prepare DataLoaders from df ---\nprint(\"Preparing DataLoaders...\")\ntrain_df_split = df[df['partition'] == 'train'].copy()\nvalid_df_split = df[df['partition'] == 'valid'].copy()\ntest_df_split = df[df['partition'] == 'test'].copy()\n\nif TRAINING_SUBSET_SIZE > 0 and len(train_df_split) > TRAINING_SUBSET_SIZE:\n    train_df_split = train_df_split.sample(n=TRAINING_SUBSET_SIZE, random_state=42)\n\ntrain_dataset = CodeSummaryDataset(train_df_split, tokenizer, MAX_CODE_LEN, MAX_SUMMARY_LEN)\nvalid_dataset = CodeSummaryDataset(valid_df_split, tokenizer, MAX_CODE_LEN, MAX_SUMMARY_LEN)\n\n# ===> THE FIX FOR ASSERTION ERRORS IS HERE <===\n# Set num_workers=0 to prevent multiprocessing errors in Kaggle.\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nprint(f\"Data ready. Training batches: {len(train_loader)}, Validation batches: {len(valid_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Task 6: Model Implementation (BiLSTM Encoder-Decoder) ---\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout, pretrained_embeddings):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n        self.embedding.weight.data.copy_(pretrained_embeddings)\n        self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout, bidirectional=True, batch_first=True)\n        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.n_layers = n_layers\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.lstm(embedded)\n        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        cell_cat = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)\n        new_hidden = torch.tanh(self.fc_hidden(hidden_cat)).unsqueeze(0).repeat(self.n_layers, 1, 1)\n        new_cell = torch.tanh(self.fc_cell(cell_cat)).unsqueeze(0).repeat(self.n_layers, 1, 1)\n        return new_hidden, new_cell\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout, pretrained_embeddings):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n        self.embedding.weight.data.copy_(pretrained_embeddings)\n        self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, input_token, hidden, cell):\n        input_token = input_token.unsqueeze(1)\n        embedded = self.dropout(self.embedding(input_token))\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder, self.decoder, self.device = encoder, decoder, device\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.shape[0], trg.shape[1]\n        trg_vocab_size = self.decoder.fc_out.out_features\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        hidden, cell = self.encoder(src)\n        input_token = trg[:, 0]\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input_token, hidden, cell)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = trg[:, t] if teacher_force else top1\n        return outputs\n\n# --- Initialize Model, Optimizer, and NEW Scheduler ---\nprint(f\"Initializing BiLSTM Seq2Seq on {DEVICE}...\")\nenc = Encoder(VOCAB_SIZE, ENCODER_EMBED_DIM, HIDDEN_DIM, LSTM_LAYERS, ENCODER_DROPOUT, pretrained_embeddings)\ndec = Decoder(VOCAB_SIZE, ENCODER_EMBED_DIM, HIDDEN_DIM, LSTM_LAYERS, DECODER_DROPOUT, pretrained_embeddings)\nmodel = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n\n# ===> ADDING THE LEARNING RATE SCHEDULER <===\n# This will reduce LR when validation loss stops improving.\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=2, verbose=True)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Model initialized. Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Task 7: Training Procedures ---\ndef train_step(model, dataloader, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    for src, trg in tqdm(dataloader, desc=\"Training\", leave=False):\n        src, trg = src.to(DEVICE), trg.to(DEVICE)\n        optimizer.zero_grad()\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(dataloader)\n\ndef evaluate_step(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, trg in tqdm(dataloader, desc=\"Validating\", leave=False):\n            src, trg = src.to(DEVICE), trg.to(DEVICE)\n            output = model(src, trg, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            trg = trg[:, 1:].reshape(-1)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n    return epoch_loss / len(dataloader)\n\n# --- Main Execution Loop (with Scheduler) ---\nprint(\"Starting Training...\")\nbest_valid_loss = float('inf')\ntrain_losses, valid_losses = [], []\nMODEL_SAVE_PATH = 'bilstm_model.pt'\n\nfor epoch in range(NUM_EPOCHS):\n    start_time = time.time()\n    train_loss = train_step(model, train_loader, optimizer, criterion, clip=1.0)\n    valid_loss = evaluate_step(model, valid_loader, criterion)\n    \n    # ===> SCHEDULER STEP <===\n    scheduler.step(valid_loss) # Pass validation loss to the scheduler\n\n    end_time = time.time()\n    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        save_msg = f\"-> Model Saved\"\n    else:\n        save_msg = \"\"\n    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s {save_msg}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.2f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):.2f}')\nprint(\"Training Complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Task 7: Comprehensive Evaluation ---\n# 1. Convergence Analysis (Plot)\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(valid_losses, label='Validation Loss')\nplt.title('Training Convergence Analysis')\nplt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\nplt.savefig('training_convergence.png'); plt.show()\n\n# ===> NEW INFERENCE FUNCTION: BEAM SEARCH <===\ndef generate_summary_beam_search(model, tokenizer, code_text, device, max_len=MAX_SUMMARY_LEN, beam_width=5):\n    model.eval()\n    with torch.no_grad():\n        tokens = tokenizer.encode(code_text)[:MAX_CODE_LEN]\n        src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n        hidden, cell = model.encoder(src_tensor)\n        \n        # Start with <SOS> token for all beams\n        # Each beam is a tuple of (sequence_of_ids, probability_score, hidden_state, cell_state)\n        beams = [([SOS_ID], 0.0, hidden, cell)]\n        \n        for _ in range(max_len):\n            new_beams = []\n            for seq, score, h, c in beams:\n                if seq[-1] == EOS_ID: # This beam is finished\n                    new_beams.append((seq, score, h, c))\n                    continue\n                \n                last_token = torch.LongTensor([seq[-1]]).to(device)\n                output, new_h, new_c = model.decoder(last_token, h, c)\n                \n                # Get top N next tokens and their probabilities\n                log_probs = torch.log_softmax(output, dim=1)\n                top_log_probs, top_indices = log_probs.topk(beam_width)\n                \n                for i in range(beam_width):\n                    next_token_id = top_indices[0, i].item()\n                    log_prob = top_log_probs[0, i].item()\n                    new_seq = seq + [next_token_id]\n                    new_score = score + log_prob\n                    new_beams.append((new_seq, new_score, new_h, new_c))\n            \n            # Sort all potential new beams by their score and keep the best `beam_width`\n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            \n            # Check if all top beams have finished\n            if all(b[0][-1] == EOS_ID for b in beams):\n                break\n                \n    # The best sequence is the one with the highest score\n    best_seq = beams[0][0]\n    generated_summary = tokenizer.decode(best_seq[1:-1]) # Exclude SOS and EOS\n    return generated_summary\n\n# 2. BLEU Score Evaluation\nprint(\"\\nStarting BLEU Score Evaluation on Test Set...\")\nmodel.load_state_dict(torch.load(MODEL_SAVE_PATH))\ntest_eval_df = test_df_split.dropna(subset=['code', 'summary']).head(1000)\npredictions, references = [], []\n\nfor _, row in tqdm(test_eval_df.iterrows(), total=len(test_eval_df), desc=\"Generating Summaries for BLEU\"):\n    code_text, reference_summary = row['code'], str(row['summary'])\n    # Call the new beam search function\n    predicted_summary = generate_summary_beam_search(model, tokenizer, code_text, DEVICE)\n    predictions.append(predicted_summary)\n    references.append(reference_summary)\n\nbleu = sacrebleu.corpus_bleu(predictions, [references])\nprint(f\"\\n>>> Final Test BLEU Score: {bleu.score:.2f} <<<\")\n\n# 3. Generation Quality Examples\nprint(\"\\n--- Qualitative Examples (Test Set) ---\")\nfor _, row in test_df_split.sample(3, random_state=123).iterrows():\n    code_in, truth = row['code'], row['summary']\n    # Call the new beam search function\n    gen_summary = generate_summary_beam_search(model, tokenizer, code_in, DEVICE)\n    print(f\"\\nInput Code Snippet:\\n{code_in[:200]}...\")\n    print(f\"Ground Truth Summary: {truth}\")\n    print(f\"Generated Summary:    {gen_summary}\")\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}