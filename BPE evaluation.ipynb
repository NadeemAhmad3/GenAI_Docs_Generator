# task3_bpe_evaluation.py
import os
import json
import pickle
import pandas as pd
import numpy as np
from collections import Counter
from typing import List, Dict, Tuple
from tqdm.auto import tqdm

# --------------------------
# Load Trained Tokenizers
# --------------------------
class BPE:
    def __init__(self):
        self.vocab = []
        self.token_to_id = {}
        self.id_to_token = {}
        self.merges = {}
        self.vocab_size = 0

    def load(self, prefix: str):
        vocab_path = f"{prefix}_vocab.json"
        merges_path = f"{prefix}_merges.json"

        with open(vocab_path, "r", encoding="utf-8") as f:
            self.token_to_id = json.load(f)
        self.vocab = [""] * len(self.token_to_id)
        for tok, idx in self.token_to_id.items():
            self.vocab[idx] = tok
        self.id_to_token = {i: t for t, i in self.token_to_id.items()}
        self.vocab_size = len(self.vocab)

        with open(merges_path, "r", encoding="utf-8") as f:
            merges_loaded = json.load(f)
        self.merges = {tuple(k.split(" ")): v for k, v in merges_loaded.items()}

    def _tokenize_word(self, word: str) -> List[str]:
        word_tuple = tuple(word) + ("</w>",)
        while True:
            pairs = list(zip(word_tuple[:-1], word_tuple[1:]))
            applicable_merges = {p: self.merges[p] for p in pairs if p in self.merges}
            if not applicable_merges:
                break
            best_pair = min(applicable_merges, key=applicable_merges.get)
            new_token = "".join(best_pair)
            new_word_tuple = []
            i = 0
            while i < len(word_tuple):
                if i < len(word_tuple) - 1 and (word_tuple[i], word_tuple[i+1]) == best_pair:
                    new_word_tuple.append(new_token)
                    i += 2
                else:
                    new_word_tuple.append(word_tuple[i])
                    i += 1
            word_tuple = tuple(new_word_tuple)
        return list(word_tuple)

    def encode(self, text: str) -> List[str]:
        tokens = []
        for word in text.strip().split():
            tokens.extend(self._tokenize_word(word))
        return tokens


# --------------------------
# Evaluation Metrics
# --------------------------
def jaccard_similarity(vocab1: set, vocab2: set) -> float:
    return len(vocab1 & vocab2) / len(vocab1 | vocab2)

def compression_ratio(raw_texts: List[str], tokenized_texts: List[List[str]]) -> float:
    raw_len = sum(len(t) for t in raw_texts)
    tok_len = sum(len(toks) for toks in tokenized_texts)
    return raw_len / tok_len if tok_len > 0 else 0

def boundary_accuracy(gt_tokens: List[List[str]], pred_tokens: List[List[str]]) -> float:
    total_boundaries, correct = 0, 0
    for gt, pred in zip(gt_tokens, pred_tokens):
        gt_boundaries = set(np.cumsum([len(t) for t in gt]))
        pred_boundaries = set(np.cumsum([len(t) for t in pred]))
        total_boundaries += len(gt_boundaries)
        correct += len(gt_boundaries & pred_boundaries)
    return correct / total_boundaries if total_boundaries > 0 else 0

def consistency_score(tokenizer: BPE, texts: List[str]) -> float:
    inconsistencies = 0
    word_map = {}
    for text in texts:
        for word in text.strip().split():
            toks = tuple(tokenizer._tokenize_word(word))
            if word in word_map and word_map[word] != toks:
                inconsistencies += 1
            word_map[word] = toks
    return 1 - (inconsistencies / max(1, len(word_map)))

def oov_rate(tokenizer: BPE, texts: List[str], gt_vocab: set) -> float:
    total, oov = 0, 0
    for text in texts:
        for word in text.strip().split():
            total += 1
            if word not in gt_vocab:
                oov += 1
    return oov / total if total > 0 else 0


# --------------------------
# Main Evaluation
# --------------------------
def main():
    # Load real dataset
    real_df = pd.read_csv("/kaggle/input/python-functions-with-docstrings/python_functions_and_documentation_dataset.csv")
    texts = real_df["code"].fillna("").tolist() + real_df["docstring"].fillna("").tolist()

    # Load trained tokenizers from temp-bpe
    paths = {
        "code_only": "/kaggle/input/temp-bpe/bpe_code_only",
        "docs_only": "/kaggle/input/temp-bpe/bpe_docs_only",
        "combined": "/kaggle/input/temp-bpe/bpe_combined",
    }

    results = {}
    # Build ground truth vocab from dataset words
    gt_vocab = set(" ".join(texts).split())

    for name, prefix in paths.items():
        print(f"\n--- Evaluating {name.upper()} tokenizer ---")

        tokenizer = BPE()
        tokenizer.load(prefix)

        # Tokenize a sample subset for evaluation
        sample_texts = texts[:2000]  # subset for speed
        tokenized = [tokenizer.encode(t) for t in sample_texts]

        # Ground truth tokens (whitespace split for reference)
        gt_tokens = [t.split() for t in sample_texts]

        metrics = {
            "Jaccard_Similarity": jaccard_similarity(set(tokenizer.vocab), gt_vocab),
            "Compression_Ratio": compression_ratio(sample_texts, tokenized),
            "Boundary_Accuracy": boundary_accuracy(gt_tokens, tokenized),
            "Consistency_Score": consistency_score(tokenizer, sample_texts),
            "OOV_Rate": oov_rate(tokenizer, sample_texts, gt_vocab),
        }

        results[name] = metrics
        for k, v in metrics.items():
            print(f"{k}: {v:.4f}")

    # --------------------------
    # Save Results
    # --------------------------
    results_df = pd.DataFrame(results).T  # rows = models
    results_df.to_csv("bpe_evaluation_results.csv", index=True)
    with open("bpe_evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2)

    print("\n=== Final Evaluation Results ===")
    print(results_df)


if __name__ == "__main__":
    main()
