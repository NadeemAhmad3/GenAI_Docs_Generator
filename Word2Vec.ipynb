{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n#\n#  FINAL SCRIPT WITH YOUR SAMPLING STRATEGY FOR MAXIMUM SPEED\n#  - YOUR STRATEGY: Samples 50k for code, 150k for docs, and a balanced 150k for combined.\n#  - This is the fastest version, designed to meet your time limits.\n#\n# ==============================================================================\n\nimport re\nimport collections\nimport json\nimport time\nimport os\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Tuple\nimport numpy as np\nimport pandas as pd\n\n# ==============================================================================\n# Part 1: BPE Tokenizer Class (Unchanged)\n# ==============================================================================\nclass BPE:\n    def __init__(self):\n        self.vocab: List[str] = []\n        self.merges: Dict[Tuple[str, str], int] = {}\n        self.token_to_id: Dict[str, int] = {}\n        self.id_to_token: Dict[int, str] = {}\n        self.vocab_size: int = 0\n    \n    def load(self, file_prefix: str):\n        vocab_path, merges_path = f\"{file_prefix}_vocab.json\", f\"{file_prefix}_merges.json\"\n        if not os.path.exists(vocab_path): raise FileNotFoundError(f\"BPE vocab file not found: {vocab_path}\")\n        if not os.path.exists(merges_path): raise FileNotFoundError(f\"BPE merges file not found: {merges_path}\")\n        with open(vocab_path, 'r', encoding='utf-8') as f: self.token_to_id = json.load(f)\n        self.vocab = [\"\"] * len(self.token_to_id)\n        for tok, idx in self.token_to_id.items(): self.vocab[idx] = tok\n        self._invert_vocab()\n        with open(merges_path, 'r', encoding='utf-8') as f: merges_loaded = json.load(f)\n        self.merges = {tuple(k.split(' ')): v for k, v in merges_loaded.items()}\n        print(f\"Successfully loaded BPE tokenizer from '{file_prefix}'. Vocab size: {self.vocab_size}\")\n\n    def _invert_vocab(self):\n        self.id_to_token = {i: t for t, i in self.token_to_id.items()}\n        self.vocab_size = len(self.vocab)\n\n    def _tokenize_word(self, word: str) -> List[str]:\n        word_tuple = tuple(word) + ('</w>',)\n        while True:\n            pairs = list(zip(word_tuple[:-1], word_tuple[1:]))\n            applicable_merges = {p: self.merges[p] for p in pairs if p in self.merges}\n            if not applicable_merges: break\n            best_pair = min(applicable_merges, key=applicable_merges.get)\n            new_token, i, new_word_tuple = \"\".join(best_pair), 0, []\n            while i < len(word_tuple):\n                if i < len(word_tuple) - 1 and (word_tuple[i], word_tuple[i+1]) == best_pair:\n                    new_word_tuple.append(new_token); i += 2\n                else: new_word_tuple.append(word_tuple[i]); i += 1\n            word_tuple = tuple(new_word_tuple)\n        return list(word_tuple)\n\n    def encode(self, text: str) -> List[int]:\n        if not isinstance(text, str): return []\n        ids, unknown_id = [], self.token_to_id.get('<UNK>')\n        words = re.findall(r\"\\w+|\\S\", text)\n        for word in words:\n            for token in self._tokenize_word(word):\n                ids.append(self.token_to_id.get(token, unknown_id))\n        return ids\n\n# ==============================================================================\n# Part 2: Word2Vec Class (Stable and Corrected)\n# ==============================================================================\nclass Word2Vec:\n    def __init__(self, vector_size=300, window=5, learning_rate=0.025, epochs=2, subsampling_threshold=1e-5):\n        self.vector_size, self.window, self.initial_lr, self.epochs = vector_size, window, learning_rate, epochs\n        self.word2id, self.id2word, self.vocab_size = None, None, None\n        self.center_vectors, self.context_vectors = None, None\n        self.neg_sampling_table, self.neg_sampling_probs, self.subsampling_probs = None, None, None\n        self.subsampling_threshold = subsampling_threshold\n\n    def build_vocab_from_bpe(self, bpe_tokenizer: BPE):\n        self.word2id, self.id2word, self.vocab_size = bpe_tokenizer.token_to_id, bpe_tokenizer.id_to_token, bpe_tokenizer.vocab_size\n        self.center_vectors = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, (self.vocab_size, self.vector_size)).astype(np.float32)\n        self.context_vectors = np.zeros((self.vocab_size, self.vector_size), dtype=np.float32)\n        self.subsampling_probs = np.zeros(self.vocab_size, dtype=np.float32)\n        print(f\"Vocabulary size: {self.vocab_size}\")\n\n    def _prepare_sampling_distributions(self, tokenized_corpus: List[List[int]]):\n        print(\"Preparing distributions for negative sampling and subsampling...\")\n        token_counts = collections.Counter(token for sentence in tokenized_corpus for token in sentence)\n        total_tokens = sum(token_counts.values())\n        \n        vocab_indices = np.array(list(token_counts.keys()))\n        counts = np.array([token_counts[i] for i in vocab_indices])\n        \n        powers = counts ** 0.75\n        self.neg_sampling_probs = powers / np.sum(powers)\n        self.neg_sampling_table = vocab_indices\n        \n        frequencies = counts / total_tokens\n        discard_probs = 1 - np.sqrt(self.subsampling_threshold / frequencies)\n        \n        self.subsampling_probs[vocab_indices] = discard_probs\n        self.subsampling_probs[self.subsampling_probs < 0] = 0\n\n    def train(self, tokenized_corpus: List[List[int]], negative_samples: int = 5):\n        self._prepare_sampling_distributions(tokenized_corpus)\n        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n        total_words, words_processed = sum(len(s) for s in tokenized_corpus), 0\n\n        for epoch in range(self.epochs):\n            print(f\"\\n--- Starting Epoch {epoch + 1}/{self.epochs} ---\")\n            for sentence in tqdm(tokenized_corpus, desc=f\"Epoch {epoch + 1}\"):\n                subsampled_sentence = [token for token in sentence if np.random.random() > self.subsampling_probs[token]]\n                \n                for i, target_id in enumerate(subsampled_sentence):\n                    current_lr = self.initial_lr * (1 - (words_processed / (total_words * self.epochs + 1)))\n                    current_lr = max(current_lr, self.initial_lr * 0.0001)\n                    \n                    start, end = max(0, i - self.window), min(len(subsampled_sentence), i + self.window + 1)\n                    \n                    for j in range(start, end):\n                        if i == j: continue\n                        context_id = subsampled_sentence[j]\n                        target_vector = self.center_vectors[target_id]\n                        \n                        ids_to_update = np.array([context_id] + np.random.choice(self.neg_sampling_table, size=negative_samples, p=self.neg_sampling_probs).tolist())\n                        labels = np.array([1] + [0] * negative_samples)\n                        vectors_to_update = self.context_vectors[ids_to_update]\n\n                        scores = vectors_to_update.dot(target_vector)\n                        scores = np.clip(scores, -10, 10)\n                        \n                        errors = sigmoid(scores) - labels\n                        grad_center = errors.reshape(1, -1).dot(vectors_to_update).flatten()\n                        grad_context = np.outer(errors, target_vector)\n                        \n                        self.center_vectors[target_id] -= current_lr * grad_center\n                        np.subtract.at(self.context_vectors, ids_to_update, current_lr * grad_context)\n\n                words_processed += len(sentence)\n    \n    def get_embedding_matrix(self): return self.center_vectors\n\n    def find_similar_tokens(self, token: str, top_n: int = 10):\n        if token not in self.word2id: print(f\"Token '{token}' not in vocabulary.\"); return\n        target_id, target_vector = self.word2id[token], self.center_vectors[target_id]\n        similarities = self.center_vectors.dot(target_vector) / (np.linalg.norm(self.center_vectors, axis=1) * np.linalg.norm(target_vector))\n        top_indices = np.argsort(similarities)[::-1][1:top_n+1]\n        print(f\"\\nTokens most similar to '{token}':\")\n        for i in top_indices: print(f\"  - {self.id2word[i]:<20} (Similarity: {similarities[i]:.4f})\")\n\n# ==============================================================================\n# Part 3: Main Orchestration Function with YOUR Sampling Strategy\n# ==============================================================================\ndef run_all_training_tasks(bpe_files_path: str):\n    # --- YOUR STRATEGY IMPLEMENTED ---\n    CODE_SAMPLE_SIZE = 50000\n    DOCS_SAMPLE_SIZE = 150000\n    COMBINED_SAMPLE_PER_CORPUS = 75000 # (75k code + 75k docs = 150k total)\n    \n    VECTOR_SIZE, WINDOW_SIZE, EPOCHS, NEGATIVE_SAMPLES = 300, 5, 2, 5\n\n    try:\n        if not isinstance(df, pd.DataFrame): print(\"Error: Global DataFrame 'df' not found.\"); return\n    except NameError: print(\"Error: Global DataFrame 'df' not found. Please run data loading cell first.\"); return\n\n    train_df_full = df[df['partition'] == 'train'].copy()\n    print(f\"Full training data has {len(train_df_full)} rows.\")\n\n    # --- Task 1: Code-Only (50k sample) ---\n    print(\"\\n\" + \"=\"*80 + \"\\n--- Task 1: Training Word2Vec on CODE (Sampled to 50k) ---\\n\" + \"=\"*80)\n    train_df_code_sample = train_df_full.sample(n=CODE_SAMPLE_SIZE, random_state=42)\n    bpe_code = BPE(); bpe_code.load(os.path.join(bpe_files_path, 'bpe_code_only'))\n    code_corpus = [bpe_code.encode(text) for text in tqdm(train_df_code_sample['code'].dropna(), desc=\"Tokenizing code\")]\n    w2v_code = Word2Vec(VECTOR_SIZE, WINDOW_SIZE, epochs=EPOCHS); w2v_code.build_vocab_from_bpe(bpe_code)\n    w2v_code.train(code_corpus, negative_samples=NEGATIVE_SAMPLES)\n    np.save(\"word2vec_code_embeddings.npy\", w2v_code.get_embedding_matrix())\n    print(\"\\nSaved code embeddings\"); w2v_code.find_similar_tokens('def')\n\n    # --- Task 2: Docs-Only (150k sample) ---\n    print(\"\\n\" + \"=\"*80 + \"\\n--- Task 2: Training Word2Vec on DOCS (Sampled to 150k) ---\\n\" + \"=\"*80)\n    train_df_docs_sample = train_df_full.sample(n=DOCS_SAMPLE_SIZE, random_state=42)\n    bpe_docs = BPE(); bpe_docs.load(os.path.join(bpe_files_path, 'bpe_docs_only'))\n    docs_corpus = [bpe_docs.encode(text) for text in tqdm(train_df_docs_sample['docstring'].dropna(), desc=\"Tokenizing docs\")]\n    w2v_docs = Word2Vec(VECTOR_SIZE, WINDOW_SIZE, epochs=EPOCHS); w2v_docs.build_vocab_from_bpe(bpe_docs)\n    w2v_docs.train(docs_corpus, negative_samples=NEGATIVE_SAMPLES)\n    np.save(\"word2vec_docs_embeddings.npy\", w2v_docs.get_embedding_matrix())\n    print(\"\\nSaved docs embeddings\"); w2v_docs.find_similar_tokens('model')\n\n    # --- Task 3: Combined (75k code + 75k docs) ---\n    print(\"\\n\" + \"=\"*80 + \"\\n--- Task 3: Training Word2Vec on COMBINED (Sampled to 150k) ---\\n\" + \"=\"*80)\n    # Sample separately to ensure a balanced corpus\n    code_sample_for_combined = train_df_full.sample(n=COMBINED_SAMPLE_PER_CORPUS, random_state=42)\n    docs_sample_for_combined = train_df_full.sample(n=COMBINED_SAMPLE_PER_CORPUS, random_state=43) # Use a different seed\n    combined_text = code_sample_for_combined['code'].dropna().tolist() + docs_sample_for_combined['docstring'].dropna().tolist()\n    \n    bpe_combined = BPE(); bpe_combined.load(os.path.join(bpe_files_path, 'bpe_combined'))\n    combined_corpus = [bpe_combined.encode(text) for text in tqdm(combined_text, desc=\"Tokenizing combined\")]\n    w2v_combined = Word2Vec(VECTOR_SIZE, WINDOW_SIZE, epochs=EPOCHS); w2v_combined.build_vocab_from_bpe(bpe_combined)\n    w2v_combined.train(combined_corpus, negative_samples=NEGATIVE_SAMPLES)\n    np.save(\"word2vec_combined_embeddings.npy\", w2v_combined.get_embedding_matrix())\n    print(\"\\nSaved combined embeddings\"); w2v_combined.find_similar_tokens('function'); w2v_combined.find_similar_tokens('return')\n\n    print(\"\\n\" + \"=\"*80 + \"\\n--- All Training Complete ---\\n\" + \"=\"*80)\n\n# ==============================================================================\n# Main Execution Block\n# ==============================================================================\ntry:\n    # !!! IMPORTANT !!!\n    # EDIT THE PATH BELOW to the location of your BPE tokenizer files dataset.\n    BPE_KAGGLE_FILES_PATH = \"/kaggle/input/temp-bpe\"\n\n    run_all_training_tasks(bpe_files_path=BPE_KAGGLE_FILES_PATH)\nexcept NameError:\n    print(\"Execution failed: Please make sure you have run the data loading cell first.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}