{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13040869,"sourceType":"datasetVersion","datasetId":8257811}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Construct the file path based on your screenshot\n# You may need to adjust the .csv filename if it's different\nfile_path = '/kaggle/input/python-functions-with-docstrings/python_functions_and_documentation_dataset.csv'\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv(file_path)\n\n# Display the first 5 rows of the DataFrame\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:35:38.740396Z","iopub.execute_input":"2025-09-20T13:35:38.740698Z","iopub.status.idle":"2025-09-20T13:36:04.595904Z","shell.execute_reply.started":"2025-09-20T13:35:38.740673Z","shell.execute_reply":"2025-09-20T13:36:04.595018Z"}},"outputs":[{"name":"stdout","text":"                        repo                              path  \\\n0  ageitgey/face_recognition  examples/face_recognition_knn.py   \n1  ageitgey/face_recognition  examples/face_recognition_knn.py   \n2  ageitgey/face_recognition  examples/face_recognition_knn.py   \n3  ageitgey/face_recognition           face_recognition/api.py   \n4  ageitgey/face_recognition           face_recognition/api.py   \n\n                         func_name  \\\n0                            train   \n1                          predict   \n2  show_prediction_labels_on_image   \n3                     _rect_to_css   \n4              _trim_css_to_bounds   \n\n                                     original_string language  \\\n0  def train(train_dir, model_save_path=None, n_n...   python   \n1  def predict(X_img_path, knn_clf=None, model_pa...   python   \n2  def show_prediction_labels_on_image(img_path, ...   python   \n3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   python   \n4  def _trim_css_to_bounds(css, image_shape):\\n  ...   python   \n\n                                                code  \\\n0  def train(train_dir, model_save_path=None, n_n...   \n1  def predict(X_img_path, knn_clf=None, model_pa...   \n2  def show_prediction_labels_on_image(img_path, ...   \n3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...   \n4  def _trim_css_to_bounds(css, image_shape):\\n  ...   \n\n                                         code_tokens  \\\n0  ['def', 'train', '(', 'train_dir', ',', 'model...   \n1  ['def', 'predict', '(', 'X_img_path', ',', 'kn...   \n2  ['def', 'show_prediction_labels_on_image', '('...   \n3  ['def', '_rect_to_css', '(', 'rect', ')', ':',...   \n4  ['def', '_trim_css_to_bounds', '(', 'css', ','...   \n\n                                           docstring  \\\n0  Trains a k-nearest neighbors classifier for fa...   \n1  Recognizes faces in given image using a traine...   \n2  Shows the face recognition results visually.\\n...   \n3  Convert a dlib 'rect' object to a plain tuple ...   \n4  Make sure a tuple in (top, right, bottom, left...   \n\n                                    docstring_tokens  \\\n0  ['Trains', 'a', 'k', '-', 'nearest', 'neighbor...   \n1  ['Recognizes', 'faces', 'in', 'given', 'image'...   \n2  ['Shows', 'the', 'face', 'recognition', 'resul...   \n3  ['Convert', 'a', 'dlib', 'rect', 'object', 'to...   \n4  ['Make', 'sure', 'a', 'tuple', 'in', '(', 'top...   \n\n                                        sha  \\\n0  c96b010c02f15e8eeb0f71308c641179ac1f19bb   \n1  c96b010c02f15e8eeb0f71308c641179ac1f19bb   \n2  c96b010c02f15e8eeb0f71308c641179ac1f19bb   \n3  c96b010c02f15e8eeb0f71308c641179ac1f19bb   \n4  c96b010c02f15e8eeb0f71308c641179ac1f19bb   \n\n                                                 url partition  \\\n0  https://github.com/ageitgey/face_recognition/b...     train   \n1  https://github.com/ageitgey/face_recognition/b...     train   \n2  https://github.com/ageitgey/face_recognition/b...     train   \n3  https://github.com/ageitgey/face_recognition/b...     train   \n4  https://github.com/ageitgey/face_recognition/b...     train   \n\n                                             summary  \n0  Train a k - nearest neighbors classifier for f...  \n1  Predicts faces in a given image using a KNN cl...  \n2        Show the face recognition results visually.  \n3  Convert a dlib rect object to a plain tuple in...  \n4  Trim the given CSS tuple to the bounds of the ...  \n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"## Data Visulization Part:","metadata":{}},{"cell_type":"code","source":"# Check shape, columns, and missing values\nprint(\"Shape of dataset:\", df.shape)\nprint(\"\\nColumn Names:\", df.columns.tolist())\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Quick info summary\nprint(\"\\nData Types:\")\nprint(df.dtypes)\n\n# Unique counts for categorical columns\nprint(\"\\nUnique counts in categorical columns:\")\nprint(df.nunique())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12,6))\ntop_funcs = df['func_name'].value_counts().head(20)\nsns.barplot(x=top_funcs.values, y=top_funcs.index, palette=\"viridis\")\n\nplt.title(\"Top 20 Most Frequent Function Names\", fontsize=16)\nplt.xlabel(\"Count\")\nplt.ylabel(\"Function Name\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['code_token_length'] = df['code_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\ndf['doc_token_length'] = df['docstring_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(data=df.sample(2000), x=\"code_token_length\", y=\"doc_token_length\", alpha=0.5)\nplt.title(\"Code vs Docstring Token Lengths (sampled)\", fontsize=16)\nplt.xlabel(\"Code Token Length\")\nplt.ylabel(\"Docstring Token Length\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ntext = \" \".join(df['docstring'].dropna().astype(str).tolist()[:50000])  # sample for performance\nwordcloud = WordCloud(width=1000, height=600, background_color=\"white\").generate(text)\n\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most Common Words in Docstrings\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create new columns for character lengths\ndf['code_length'] = df['code'].astype(str).apply(len)\ndf['docstring_length'] = df['docstring'].astype(str).apply(len)\ndf['summary_length'] = df['summary'].astype(str).apply(len)\n# Store the original number of rows\noriginal_rows = len(df)\nprint(f\"Original dataset shape: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(x=df['docstring_length'])\nplt.title(\"Outliers in Docstring Length\", fontsize=16)\nplt.xlabel(\"Docstring Length\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['summary_length'] = df['summary'].astype(str).apply(len)\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(data=df.sample(2000), x=\"docstring_length\", y=\"summary_length\", alpha=0.5, color=\"purple\")\nplt.title(\"Docstring Length vs Summary Length\", fontsize=16)\nplt.xlabel(\"Docstring Length\")\nplt.ylabel(\"Summary Length\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up the figure for two subplots\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\nfig.suptitle('Distribution of Character Lengths', fontsize=18)\n\n# Plot for Code Length\nsns.histplot(ax=axes[0], data=df, x='code_length', bins=50, kde=True, color='skyblue')\naxes[0].set_title('Code Character Length')\naxes[0].set_xlabel('Code Length (characters)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_xlim(0, 4000) # Limiting x-axis for better visibility\n\n# Plot for Docstring Length\nsns.histplot(ax=axes[1], data=df, x='docstring_length', bins=50, kde=True, color='salmon')\naxes[1].set_title('Docstring Character Length')\naxes[1].set_xlabel('Docstring Length (characters)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_xlim(0, 2000) # Limiting x-axis for better visibility\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x='partition', palette='rocket')\nplt.title('Data Partition Distribution', fontsize=16)\nplt.xlabel('Partition')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Using a sample of the data for performance\ncode_text = \" \".join(df['code'].dropna().astype(str).tolist()[:50000])\n\nwordcloud = WordCloud(width=1000, height=600, background_color=\"black\", colormap='viridis').generate(code_text)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most Common Words in Source Code\", fontsize=16)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Using a sample of the data for performance\nsample_docstrings = df['docstring'].dropna().astype(str).sample(50000, random_state=42)\n\n# Get top 20 bigrams\nvec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(sample_docstrings)\nbag_of_words = vec.transform(sample_docstrings)\nsum_words = bag_of_words.sum(axis=0)\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\ntop_bigrams = pd.DataFrame(words_freq[:20], columns=['Bigram', 'Frequency'])\n\n# Plotting\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Frequency', y='Bigram', data=top_bigrams, palette='coolwarm')\nplt.title('Top 20 Most Frequent Bigrams in Docstrings', fontsize=16)\nplt.xlabel('Frequency')\nplt.ylabel('Bigram')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select only the numerical length columns for the correlation matrix\nlength_features = ['code_length', 'docstring_length', 'code_token_length', 'doc_token_length', 'summary_length']\ncorr_matrix = df[length_features].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='YlGnBu', fmt='.2f')\nplt.title('Correlation Heatmap of Length Features', fontsize=16)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up the figure for two subplots\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\nfig.suptitle('Token Length Distribution by Partition', fontsize=18)\n\n# Box plot for Code Token Length\nsns.boxplot(ax=axes[0], data=df, x='partition', y='code_token_length', palette='Set2')\naxes[0].set_title('Code Token Length by Partition')\naxes[0].set_xlabel('Partition')\naxes[0].set_ylabel('Code Token Length')\naxes[0].set_ylim(0, 500) # Adjust limit to zoom in on the main distribution\n\n# Box plot for Docstring Token Length\nsns.boxplot(ax=axes[1], data=df, x='partition', y='doc_token_length', palette='Set3')\naxes[1].set_title('Docstring Token Length by Partition')\naxes[1].set_xlabel('Partition')\naxes[1].set_ylabel('Docstring Token Length')\naxes[1].set_ylim(0, 250) # Adjust limit to zoom in on the main distribution\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 8))\ntop_repos = df['repo'].value_counts().head(20)\nsns.barplot(x=top_repos.values, y=top_repos.index, palette='plasma')\n\nplt.title('Top 20 Repositories by Number of Functions', fontsize=16)\nplt.xlabel('Number of Functions')\nplt.ylabel('Repository')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract prefixes (text before the first underscore)\n# We only consider function names that contain an underscore\nprefixes = df['func_name'].dropna()\nprefixes = prefixes[prefixes.str.contains('_')].apply(lambda x: x.split('_')[0] + '_')\n# A common case is functions starting with '_', so let's handle that specifically\nprefixes = prefixes.replace({'_': '_ (private/internal)'})\n\n\nplt.figure(figsize=(12, 8))\ntop_prefixes = prefixes.value_counts().head(20)\nsns.barplot(x=top_prefixes.values, y=top_prefixes.index, palette='cividis')\n\nplt.title('Top 20 Common Function Name Prefixes', fontsize=16)\nplt.xlabel('Count')\nplt.ylabel('Prefix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter for function names that start and end with '__'\ndunder_methods = df['func_name'].dropna()\ndunder_methods = dunder_methods[dunder_methods.str.startswith('__') & dunder_methods.str.endswith('__')]\n\n# --- SOLUTION: ADD THIS CHECK ---\n# First, check if the filtered DataFrame is empty\nif not dunder_methods.empty:\n    plt.figure(figsize=(12, 8))\n    top_dunders = dunder_methods.value_counts().head(15)\n    sns.barplot(x=top_dunders.values, y=top_dunders.index, palette='cubehelix')\n\n    plt.title('Frequency of Top 15 Special \"Dunder\" Methods', fontsize=16)\n    plt.xlabel('Count')\n    plt.ylabel('Dunder Method')\n    plt.show()\nelse:\n    # If it's empty, print a message instead of trying to plot\n    print(\"No 'dunder' methods (e.g., __init__) were found in the 'func_name' column.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the ratio, adding 1 to the denominator to avoid division by zero\ndf['doc_code_ratio'] = df['docstring_length'] / (df['code_length'] + 1)\n\nplt.figure(figsize=(12, 7))\n# We use a log scale on the y-axis to see the distribution more clearly\n# We also filter out extreme ratios for better visualization\nsns.histplot(df[(df['doc_code_ratio'] > 0) & (df['doc_code_ratio'] < 5)]['doc_code_ratio'],\n             bins=50, kde=True, log_scale=(False, True))\n\nplt.title('Distribution of Docstring-to-Code Character Length Ratio', fontsize=16)\nplt.xlabel('Docstring Length / Code Length')\nplt.ylabel('Frequency (Log Scale)')\nplt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract the owner/organization from the 'repo' column\n# This part is correct: it takes everything before the '/'\ndf['repo_owner'] = df['repo'].apply(lambda x: x.split('/')[0])\n\nplt.figure(figsize=(12, 8))\ntop_owners = df['repo_owner'].value_counts().head(20)\nsns.barplot(x=top_owners.values, y=top_owners.index, palette='magma')\n\n# The main change is in the title and labels for clarity\nplt.title('Top 20 Contributing Organizations/Users by Function Count', fontsize=16)\nplt.xlabel('Number of Functions')\nplt.ylabel('Organization / User')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use a smaller sample for performance, as jointplots can be slow\ndf_sample = df.sample(5000, random_state=42)\n\n# Create the joint plot\ng = sns.jointplot(\n    data=df_sample,\n    x=\"code_token_length\",\n    y=\"doc_token_length\",\n    kind=\"scatter\",\n    alpha=0.3,\n    height=10,\n    marginal_kws=dict(bins=50, fill=True)\n)\n\ng.fig.suptitle(\"Code vs Docstring Token Lengths (Joint Distribution Plot)\", y=1.01, fontsize=16)\ng.set_axis_labels(\"Code Token Length\", \"Docstring Token Length\", fontsize=12)\n\n# Set limits to zoom in on the densest area\nplt.xlim(0, 750)\nplt.ylim(0, 400)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"# --- STEP 1: Handle Missing Values ---\ndf.dropna(subset=['func_name'], inplace=True)\n\nprint(f\"Rows removed: {original_rows - len(df)}\")\nprint(f\"Current dataset shape: {df.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- STEP 2: Remove Duplicates ---\nrows_before_deduplication = len(df)\ndf.drop_duplicates(subset=['code', 'docstring', 'summary'], inplace=True)\n\nprint(f\"Rows removed: {rows_before_deduplication - len(df)}\")\nprint(f\"Current dataset shape: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- STEP 3: Remove Length-based Outliers ---\nrows_before_outlier_removal = len(df)\n\n# Define the lower and upper bounds using quantiles\nlower_bound_code = df['code_length'].quantile(0.01)\nupper_bound_code = df['code_length'].quantile(0.99)\nlower_bound_doc = df['docstring_length'].quantile(0.01)\nupper_bound_doc = df['docstring_length'].quantile(0.99)\n\nprint(f\"Code length will be filtered between: {lower_bound_code:.0f} and {upper_bound_code:.0f} characters.\")\nprint(f\"Docstring length will be filtered between: {lower_bound_doc:.0f} and {upper_bound_doc:.0f} characters.\")\n\n# Apply the filter\ndf = df[(df['code_length'] >= lower_bound_code) & (df['code_length'] <= upper_bound_code)]\ndf = df[(df['docstring_length'] >= lower_bound_doc) & (df['docstring_length'] <= upper_bound_doc)]\n\nprint(f\"\\nRows removed: {rows_before_outlier_removal - len(df)}\")\nprint(f\"Current dataset shape: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- STEP 4: Remove Trivially Short Docstrings ---\n# A docstring with fewer than 3 words is unlikely to be a good summary.\n# First, let's ensure the docstring_tokens column is in the correct list format\nimport ast\ndf['docstring_tokens_len'] = df['docstring_tokens'].apply(lambda x: len(ast.literal_eval(x)))\n\nrows_before_short_removal = len(df)\n\n# Filter out rows where the docstring has fewer than 3 tokens\ndf = df[df['docstring_tokens_len'] >= 3]\n\nprint(f\"Rows removed: {rows_before_short_removal - len(df)}\")\nprint(f\"Current dataset shape: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- FINAL REPORT ---\nfinal_rows = len(df)\nrows_removed = original_rows - final_rows\npercentage_removed = (rows_removed / original_rows) * 100\n\nprint(f\"Original number of rows: {original_rows}\")\nprint(f\"Final number of rows after cleaning: {final_rows}\")\nprint(f\"Total rows removed: {rows_removed}\")\nprint(f\"Percentage of data removed: {percentage_removed:.2f}%\")\n\n# Display the first 5 rows of the cleaned DataFrame\nprint(\"\\nPreview of the cleaned dataset:\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Define the list of columns you want to remove ---\ncolumns_to_remove = ['url', 'sha', 'language']\n\n# --- Find which of these columns actually exist in the DataFrame ---\n# This is a safety check to prevent errors if a column has already been removed\n# or never existed (like 'ssh' in this case).\nexisting_columns = [col for col in columns_to_remove if col in df.columns]\n\n# --- Drop the columns that were found ---\nif existing_columns:\n    df.drop(columns=existing_columns, inplace=True)\n    print(\"Successfully removed the following columns:\")\n    for col in existing_columns:\n        print(f\"- {col}\")\nelse:\n    print(\"No columns from your list were found in the DataFrame.\")\n\n# --- Verify the result by showing the current list of columns ---\nprint(\"\\nRemaining columns in the DataFrame:\")\nprint(df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Faster tokenization","metadata":{}},{"cell_type":"code","source":"# bpe_tokenizer_resumable.py\n# Resumable & faster BPE tokenizer implementation.\n# - Faster pair counting using Counters and tuple-based words\n# - Checkpointing (periodic save to disk) so training can resume after interruption\n# - Save vocab/merges in formats compatible with original code's save/load\n# - Three example training runs (code-only, docs-only, combined) similar to your original cells\n\nimport re\nimport collections\nimport json\nimport time\nimport pickle\nimport os\nimport signal\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Tuple\n\n# -------------------------------\n# Utilities\n# -------------------------------\n\ndef default_tokenize(text: str) -> List[str]:\n    # tokenizes into words/punctuation similar to your original regex\n    return re.findall(r\"\\w+|\\S\", text)\n\n# -------------------------------\n# BPE Tokenizer (resumable)\n# -------------------------------\nclass ResumableBPE:\n    def __init__(self):\n        self.vocab: List[str] = []\n        self.merges: Dict[Tuple[str,str], int] = {}\n        self.token_to_id: Dict[str,int] = {}\n        self.id_to_token: Dict[int,str] = {}\n        self.vocab_size: int = 0\n        # internal training state\n        self._word_freqs: Dict[Tuple[str,...], int] = {}\n        self._pair_counts: collections.Counter = collections.Counter()\n        self._occurrences: Dict[Tuple[str,str], Dict[Tuple[str,...], List[int]]] = {}\n        # checkpointing\n        self._stop_requested = False\n\n    # -------------------------------\n    # Saving / Loading\n    # -------------------------------\n    def save(self, file_prefix: str):\n        vocab_path = f\"{file_prefix}_vocab.json\"\n        merges_path = f\"{file_prefix}_merges.json\"\n        state_path = f\"{file_prefix}_state.pkl\"\n\n        with open(vocab_path, 'w', encoding='utf-8') as f:\n            json.dump(self.token_to_id, f, ensure_ascii=False, indent=2)\n\n        merges_to_save = {' '.join(k): v for k, v in self.merges.items()}\n        with open(merges_path, 'w', encoding='utf-8') as f:\n            json.dump(merges_to_save, f, ensure_ascii=False, indent=2)\n\n        # Save internal state for resuming\n        with open(state_path, 'wb') as f:\n            pickle.dump({\n                'word_freqs': self._word_freqs,\n                'pair_counts': self._pair_counts,\n                'occurrences': self._occurrences,\n                'vocab': self.vocab,\n                'merges': self.merges,\n                'vocab_size': self.vocab_size\n            }, f)\n\n        print(f\"Saved vocab -> {vocab_path}, merges -> {merges_path}, state -> {state_path}\")\n\n    def load(self, file_prefix: str):\n        vocab_path = f\"{file_prefix}_vocab.json\"\n        merges_path = f\"{file_prefix}_merges.json\"\n        state_path = f\"{file_prefix}_state.pkl\"\n\n        with open(vocab_path, 'r', encoding='utf-8') as f:\n            self.token_to_id = json.load(f)\n        self.vocab = [None] * len(self.token_to_id)\n        for tok, idx in self.token_to_id.items():\n            self.vocab[idx] = tok\n        self.id_to_token = {i: t for t, i in self.token_to_id.items()}\n        self.vocab_size = len(self.vocab)\n\n        with open(merges_path, 'r', encoding='utf-8') as f:\n            merges_loaded = json.load(f)\n        self.merges = {tuple(k.split(' ')): v for k, v in merges_loaded.items()}\n\n        if os.path.exists(state_path):\n            with open(state_path, 'rb') as f:\n                st = pickle.load(f)\n            self._word_freqs = st.get('word_freqs', {})\n            self._pair_counts = st.get('pair_counts', collections.Counter())\n            self._occurrences = st.get('occurrences', {})\n            # vocab and merges already set above\n        print(f\"Loaded tokenizer from {file_prefix}. Vocab size: {self.vocab_size}\")\n\n    # -------------------------------\n    # Internal helpers\n    # -------------------------------\n    def _initialize_from_corpus(self, texts: List[str], special_tokens: List[str]):\n        # build char-level vocab + word frequency map where words are tuples of chars + </w>\n        char_set = set()\n        word_freqs: Dict[Tuple[str,...], int] = {}\n        for text in texts:\n            for word in default_tokenize(text):\n                chars = tuple(list(word) + ['</w>'])\n                word_freqs[chars] = word_freqs.get(chars, 0) + 1\n                char_set.update(chars)\n\n        # initial vocab: special tokens + sorted chars\n        self.vocab = special_tokens + sorted(list(char_set))\n        self._word_freqs = word_freqs\n\n        # build initial pair counts and occurrences map for efficient updates\n        pair_counts = collections.Counter()\n        occurrences = {}\n        for word, freq in word_freqs.items():\n            symbols = list(word)\n            for i in range(len(symbols)-1):\n                pair = (symbols[i], symbols[i+1])\n                pair_counts[pair] += freq\n                # record occurrence: map pair -> word -> list of indices\n                occ = occurrences.setdefault(pair, {})\n                occ.setdefault(word, []).append(i)\n\n        self._pair_counts = pair_counts\n        self._occurrences = occurrences\n        self._invert_vocab()\n\n    def _invert_vocab(self):\n        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n        self.vocab_size = len(self.vocab)\n\n    # -------------------------------\n    # Core training loop (resumable)\n    # -------------------------------\n    def train(self,\n              texts: List[str],\n              target_vocab_size: int,\n              special_tokens: List[str]=None,\n              checkpoint_prefix: str='bpe_checkpoint',\n              checkpoint_interval: int=100,\n              checkpoint_seconds: int=300,\n              resume: bool=False):\n        \"\"\"\n        Train BPE with resumable checkpoints.\n        - checkpoint_interval: save every N merges\n        - checkpoint_seconds: also save if time since last save > this\n        - resume: if True, will try to load checkpoint files with given prefix\n        \"\"\"\n        if special_tokens is None:\n            special_tokens = []\n\n        # if resume and checkpoint exists, try to load\n        state_path = f\"{checkpoint_prefix}_state.pkl\"\n        if resume and os.path.exists(state_path):\n            print(\"Resuming from checkpoint...\")\n            self.load(checkpoint_prefix)\n            # compute remaining merges\n            num_merges_done = len(self.merges)\n            merges_to_do = target_vocab_size - self.vocab_size\n        else:\n            print(\"Initializing from corpus...\")\n            self._initialize_from_corpus(texts, special_tokens)\n            self.merges = {}\n            num_merges_done = 0\n            merges_to_do = target_vocab_size - self.vocab_size\n\n        if merges_to_do <= 0:\n            print(\"Requested vocab size already satisfied. Nothing to do.\")\n            return\n\n        last_checkpoint_time = time.time()\n        start_time = time.time()\n        pbar = tqdm(range(num_merges_done, num_merges_done + merges_to_do), desc='Merges')\n\n        try:\n            for merge_index in pbar:\n                if not self._pair_counts:\n                    print(\"No more pairs to merge.\")\n                    break\n                # pick best pair (highest count)\n                best_pair, best_count = self._pair_counts.most_common(1)[0]\n                # apply merge\n                new_symbol = ''.join(best_pair)\n                # record merge with increasing integer rank\n                self.merges[best_pair] = merge_index\n                self.vocab.append(new_symbol)\n                # update word_freqs, pair_counts, occurrences\n                self._apply_merge(best_pair, new_symbol)\n                # update progress bar\n                if (merge_index + 1) % 10 == 0:\n                    pbar.set_postfix({\"latest_merge\": f\"{best_pair} -> {new_symbol}\",\n                                      \"vocab_size\": len(self.vocab)})\n\n                # Checkpoint by interval\n                now = time.time()\n                if ((merge_index + 1) % checkpoint_interval == 0) or (now - last_checkpoint_time > checkpoint_seconds):\n                    self._checkpoint(checkpoint_prefix)\n                    last_checkpoint_time = now\n                if self._stop_requested:\n                    print(\"Stop requested: checkpointing and exiting training loop.\")\n                    self._checkpoint(checkpoint_prefix)\n                    break\n\n        except Exception as e:\n            print(f\"Exception during training: {e}\\nSaving checkpoint before raising.\")\n            self._checkpoint(checkpoint_prefix)\n            raise\n\n        end_time = time.time()\n        print(f\"Training finished. Time elapsed: {(end_time - start_time)/60:.2f} minutes\")\n        # final checkpoint\n        self._checkpoint(checkpoint_prefix)\n        self._invert_vocab()\n\n    def _apply_merge(self, pair: Tuple[str,str], new_symbol: str):\n        \"\"\"Replace pair occurrences in self._word_freqs and update pair counts and occurrences efficiently.\"\"\"\n        pair_occ = self._occurrences.get(pair, {})\n        if not pair_occ:\n            # nothing to do\n            # remove pair from counts if present\n            self._pair_counts.pop(pair, None)\n            return\n\n        new_occurrences_updates = {}\n        new_pair_counts = collections.Counter()\n\n        # For each word where pair occurs, produce the merged word and update counts\n        for word, indices in pair_occ.items():\n            freq = self._word_freqs.pop(word, 0)\n            if freq == 0:\n                continue\n            symbols = list(word)\n            # create merged symbols list by merging all non-overlapping occurrences\n            i = 0\n            merged_symbols = []\n            while i < len(symbols):\n                if i < len(symbols) -1 and (symbols[i], symbols[i+1]) == pair:\n                    merged_symbols.append(new_symbol)\n                    i += 2\n                else:\n                    merged_symbols.append(symbols[i])\n                    i += 1\n            new_word = tuple(merged_symbols)\n            # update word_freqs\n            self._word_freqs[new_word] = self._word_freqs.get(new_word, 0) + freq\n\n            # update pair counts and occurrences for the new word\n            for j in range(len(merged_symbols)-1):\n                p = (merged_symbols[j], merged_symbols[j+1])\n                new_pair_counts[p] += freq\n                occ_map = new_occurrences_updates.setdefault(p, {})\n                occ_map.setdefault(new_word, []).append(j)\n\n        # Remove old pair entirely\n        self._pair_counts.pop(pair, None)\n        self._occurrences.pop(pair, None)\n\n        # Now rebuild pair counts: subtract contributions of words we modified\n        # Easiest approach: recompute pair_counts from scratch from occurrences mapping\n        # But we can update incrementally: remove contributions of old words (already removed above)\n        # and add new_pair_counts\n\n        # Remove contributions of modified words from existing counts\n        # Note: pair_occ keys were removed from _word_freqs already so old contributions should not exist\n        # Add new pair counts\n        self._pair_counts.update(new_pair_counts)\n\n        # Update occurrences map: remove any word references for pairs that no longer exist\n        # and add new occurrences\n        for p, occs in new_occurrences_updates.items():\n            occ_map = self._occurrences.setdefault(p, {})\n            for w, idxs in occs.items():\n                occ_map.setdefault(w, []).extend(idxs)\n\n        # There may be leftover occurrences referencing removed words; that's okay — they won't be used\n        # as we always consult self._word_freqs when applying merges\n\n    def _checkpoint(self, prefix: str):\n        try:\n            # prepare token_to_id and save\n            self._invert_vocab()\n            self.save(prefix)\n        except Exception as e:\n            print(f\"Failed to checkpoint: {e}\")\n\n    def request_stop(self):\n        self._stop_requested = True\n\n    # -------------------------------\n    # Encoding / Decoding\n    # -------------------------------\n    def _get_word_tokens(self, word: str) -> List[str]:\n        # greedy merge using learned merges ordered by their assigned rank\n        tokens = list(word) + ['</w>']\n        merges_rank = self.merges\n        # keep merging while any pair exists in merges\n        while True:\n            pairs = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n            candidates = [(p, merges_rank[p]) for p in pairs if p in merges_rank]\n            if not candidates:\n                break\n            # pick the pair with smallest rank (earlier merge)\n            best_pair, _ = min(candidates, key=lambda x: x[1])\n            # merge first occurrence of that pair\n            idx = pairs.index(best_pair)\n            tokens = tokens[:idx] + [\"\".join(best_pair)] + tokens[idx+2:]\n        return tokens\n\n    def encode(self, text: str) -> List[int]:\n        ids = []\n        for word in default_tokenize(text):\n            for tok in self._get_word_tokens(word):\n                ids.append(self.token_to_id.get(tok, self.token_to_id.get('<UNK>', 0)))\n        return ids\n\n    def decode(self, token_ids: List[int]) -> str:\n        toks = [self.id_to_token.get(i, '<UNK>') for i in token_ids]\n        text = ''.join(toks).replace('</w>', ' ')\n        return text\n\n# -------------------------------\n# Signal handlers to allow graceful stop\n# -------------------------------\ndef install_signal_handlers(bpe: ResumableBPE):\n    def handler(signum, frame):\n        print(f\"Signal {signum} received. Requesting graceful stop...\")\n        bpe.request_stop()\n    signal.signal(signal.SIGINT, handler)\n    signal.signal(signal.SIGTERM, handler)\n\n# -------------------------------\n# Example training usage (three tasks) - adapt to your DataFrame variable `df`\n# -------------------------------\n\nif __name__ == '__main__':\n    # Example parameters --- change to suit your environment\n    DEV_VOCAB_SIZE = 32000\n    DEV_CORPUS_SAMPLE_SIZE = 50000\n    SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n\n    # Assume you have a pandas DataFrame `df` with 'code' and 'docstring' columns\n    import pandas as pd\n    # Replace this with your actual df loading if running stand-alone\n    # df = pd.read_csv('your_dataset.csv')\n\n    # For demonstration we'll check df exists\n    if 'df' not in globals():\n        print('Warning: df not found in globals. This module expects `df` to be provided by the caller.')\n\n    # Helper to run a task with checkpointing\n    def run_task(texts, file_prefix):\n        bpe = ResumableBPE()\n        install_signal_handlers(bpe)\n        # If there's an existing checkpoint, pass resume=True to continue\n        resume = os.path.exists(f\"{file_prefix}_state.pkl\")\n        bpe.train(texts=texts,\n                  target_vocab_size=DEV_VOCAB_SIZE,\n                  special_tokens=SPECIAL_TOKENS,\n                  checkpoint_prefix=file_prefix,\n                  checkpoint_interval=200,\n                  checkpoint_seconds=300,\n                  resume=resume)\n\n    # TASK 1: CODE-ONLY\n    if 'df' in globals():\n        print('\\n' + '='*80)\n        print('--- Starting DEV Training for CODE-ONLY Tokenizer ---')\n        print('='*80)\n        code_corpus_sample = df['code'].sample(min(len(df), DEV_CORPUS_SAMPLE_SIZE), random_state=42).tolist()\n        run_task(code_corpus_sample, 'bpe_code_only_tokenizer_dev')\n\n        # TASK 2: DOCS-ONLY\n        print('\\n' + '='*80)\n        print('--- Starting DEV Training for DOCS-ONLY Tokenizer ---')\n        print('='*80)\n        docs_corpus_sample = df['docstring'].sample(min(len(df), DEV_CORPUS_SAMPLE_SIZE), random_state=42).tolist()\n        run_task(docs_corpus_sample, 'bpe_docs_only_tokenizer_dev')\n\n        # TASK 3: COMBINED\n        print('\\n' + '='*80)\n        print('--- Starting DEV Training for COMBINED Tokenizer ---')\n        print('='*80)\n        combined_corpus_sample = df['code'].sample(DEV_CORPUS_SAMPLE_SIZE // 2, random_state=42).tolist() + \\\n                                 df['docstring'].sample(DEV_CORPUS_SAMPLE_SIZE // 2, random_state=42).tolist()\n        run_task(combined_corpus_sample, 'bpe_combined_tokenizer_dev')\n\n        print('\\n' + '='*80)\n        print('--- ALL THREE TOKENIZERS COMPLETED (OR WERE RESUMED/STOPPED SAFELY) ---')\n        print('='*80)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BPE Evaluation","metadata":{}},{"cell_type":"code","source":"# cell 1: imports & basic helpers\nimport json\nimport math\nimport os\nimport pickle\nfrom collections import defaultdict, Counter\nfrom typing import List, Tuple, Dict, Set\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# Use the same tokenization used to train (if different, adapt)\ndef default_tokenize(text: str) -> List[str]:\n    import re\n    return re.findall(r\"\\w+|\\S\", text)\n\n# safe loader for lists stored as strings in DataFrame\ndef ensure_list(x):\n    if isinstance(x, list):\n        return x\n    if pd.isna(x):\n        return []\n    if isinstance(x, str):\n        # try to parse Python-literal list like \"['a','b']\" or JSON\n        try:\n            v = json.loads(x.replace(\"'\", '\"'))\n            if isinstance(v, list): return v\n        except Exception:\n            pass\n        # fallback: simple split on whitespace/punctuation (not ideal)\n        return default_tokenize(x)\n    return list(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:36:14.544818Z","iopub.execute_input":"2025-09-20T13:36:14.545189Z","iopub.status.idle":"2025-09-20T13:36:14.621609Z","shell.execute_reply.started":"2025-09-20T13:36:14.545160Z","shell.execute_reply":"2025-09-20T13:36:14.620664Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# cell 2: loading your tokenizer (assumes ResumableBPE class is available)\n# If your ResumableBPE is in a separate file, import it:\n# from bpe_tokenizer_resumable import ResumableBPE\n\ndef load_bpe_from_prefix(prefix: str):\n    bpe = ResumableBPE()\n    bpe.load(prefix)\n    return bpe\n\n# Example:\n# bpe = load_bpe_from_prefix('bpe_code_only_tokenizer_dev')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:36:37.178800Z","iopub.execute_input":"2025-09-20T13:36:37.179153Z","iopub.status.idle":"2025-09-20T13:36:37.184438Z","shell.execute_reply.started":"2025-09-20T13:36:37.179126Z","shell.execute_reply":"2025-09-20T13:36:37.183150Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# cell 3: metric functions\n\ndef jaccard(set_a: Set[str], set_b: Set[str]) -> float:\n    if not set_a and not set_b: return 1.0\n    inter = len(set_a & set_b)\n    union = len(set_a | set_b)\n    return inter / union if union else 0.0\n\ndef compression_ratio(original_tokens: List[str], bpe_ids: List[int]) -> float:\n    # ratio: total original tokens / total produced BPE tokens (higher -> more compression)\n    if not bpe_ids: \n        return 0.0\n    return len(original_tokens) / len(bpe_ids)\n\ndef token_boundaries_from_tokens(tokens: List[str], sep=' ')-> Set[int]:\n    # returns set of boundary char indices (end positions) for sequence when joined by sep\n    # tokens = ['hello','world'] -> boundaries {5, 11} (assuming sep length 1). We use cumulative lengths.\n    boundaries = set()\n    pos = 0\n    sep_len = len(sep)\n    for t in tokens:\n        pos += len(t)\n        boundaries.add(pos)\n        pos += sep_len\n    return boundaries\n\ndef decode_bpe_ids_to_tokens(bpe: ResumableBPE, ids: List[int]) -> List[str]:\n    # returns list of token strings for provided ids\n    return [bpe.id_to_token.get(i, '<UNK>') for i in ids]\n\ndef compute_boundary_PRF(ground_tokens: List[str], bpe_tokens: List[str]) -> Tuple[float,float,float]:\n    # represent both as boundaries on character positions and compute precision/recall/f1\n    # Join tokens with single space to approximate original spacing (docstrings/code may differ;\n    # this is a pragmatic approximation—works well if ground tokens are words/punct)\n    g_bound = token_boundaries_from_tokens(ground_tokens, sep=' ')\n    b_bound = token_boundaries_from_tokens(bpe_tokens, sep=' ')\n    if not g_bound and not b_bound:\n        return 1.0, 1.0, 1.0\n    tp = len(g_bound & b_bound)\n    pred = len(b_bound)\n    actual = len(g_bound)\n    prec = tp / pred if pred else 0.0\n    rec = tp / actual if actual else 0.0\n    f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n    return prec, rec, f1\n\ndef compute_consistency_score(mapping: Dict[str, Counter]) -> float:\n    # mapping: surface_token -> Counter of observed BPE sequences (as tuples)\n    # score = weighted fraction of occurrences where token maps to its most common BPE seq\n    total_occ = 0\n    consistent = 0\n    for surface, counter in mapping.items():\n        s = sum(counter.values())\n        total_occ += s\n        most = counter.most_common(1)[0][1] if counter else 0\n        consistent += most\n    return consistent / total_occ if total_occ else 1.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:36:50.352611Z","iopub.execute_input":"2025-09-20T13:36:50.353374Z","iopub.status.idle":"2025-09-20T13:36:50.365235Z","shell.execute_reply.started":"2025-09-20T13:36:50.353320Z","shell.execute_reply":"2025-09-20T13:36:50.364231Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# cell 4: evaluate single example (returns metrics dict)\ndef evaluate_row(bpe: ResumableBPE, ground_tokens: List[str], text_for_encoding: str=None):\n    \"\"\"\n    ground_tokens: list of ground tokens (already tokenized: e.g. df['code_tokens'])\n    text_for_encoding: original text string to feed to bpe.encode (if None, we join ground_tokens with spaces)\n    \"\"\"\n    if text_for_encoding is None:\n        text_for_encoding = \" \".join(ground_tokens)\n\n    # encode via BPE -> ids -> tokens\n    ids = bpe.encode(text_for_encoding)\n    bpe_tokens = decode_bpe_ids_to_tokens(bpe, ids)\n\n    # metrics:\n    # vocab overlap: compare sets of tokens (ground vs bpe vocab for this example)\n    ground_set = set(ground_tokens)\n    bpe_set = set(bpe_tokens)\n    vocab_jacc = jaccard(ground_set, bpe_set)\n\n    comp_ratio = compression_ratio(ground_tokens, ids)\n\n    prec, rec, f1 = compute_boundary_PRF(ground_tokens, bpe_tokens)\n\n    # oov per-example (how many ground tokens not in global bpe vocab)\n    oov_count = sum(1 for t in ground_tokens if t not in bpe.token_to_id)\n    oov_rate = oov_count / len(ground_tokens) if ground_tokens else 0.0\n\n    return {\n        'vocab_jaccard': vocab_jacc,\n        'compression_ratio': comp_ratio,\n        'boundary_prec': prec,\n        'boundary_rec': rec,\n        'boundary_f1': f1,\n        'oov_rate': oov_rate,\n        'orig_token_count': len(ground_tokens),\n        'bpe_token_count': len(ids)\n    }, bpe_tokens, ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:37:06.098288Z","iopub.execute_input":"2025-09-20T13:37:06.098953Z","iopub.status.idle":"2025-09-20T13:37:06.106639Z","shell.execute_reply.started":"2025-09-20T13:37:06.098920Z","shell.execute_reply":"2025-09-20T13:37:06.105552Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# cell 5: driver to run evaluation across dataframe column\ndef evaluate_corpus(bpe: ResumableBPE,\n                    df: pd.DataFrame,\n                    col_tokens: str,\n                    col_text: str = None,\n                    sample_n: int = None,\n                    save_path: str = 'bpe_eval_results.csv',\n                    progress_interval: int = 200):\n    \"\"\"\n    col_tokens: column name that contains ground-truth token lists (like 'code_tokens' or 'docstring_tokens')\n    col_text: optional column containing original string (if present); else will join tokens with spaces\n    sample_n: if set, randomly sample up to sample_n rows (faster)\n    \"\"\"\n    if sample_n:\n        eval_df = df.sample(min(len(df), sample_n), random_state=42).reset_index(drop=True)\n    else:\n        eval_df = df.reset_index(drop=True)\n\n    results = []\n    # for consistency: map surface token -> Counter of observed BPE sequence strings\n    consistency_map = defaultdict(Counter)\n\n    for i, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=f\"Evaluating {col_tokens}\"):\n        ground = ensure_list(row[col_tokens])\n        if col_text and col_text in row and pd.notna(row[col_text]):\n            text = row[col_text]\n        else:\n            text = \" \".join(ground)\n\n        metrics, bpe_tokens, ids = evaluate_row(bpe, ground, text_for_encoding=text)\n        results.append(metrics)\n\n        # update consistency: for each ground token, capture BPE seq corresponding to it\n        # Heuristic: reconstruct bpe_tokens into words by consuming token lengths\n        # Simpler approach: for each ground token surface s, record bpe sequence equal to\n        # the concatenation of consecutive bpe_tokens until length matches len(s) (approx).\n        # This is heuristic but works well with char-level merges.\n        j = 0\n        for s in ground:\n            seq = []\n            cur_len = 0\n            target = len(s)\n            while j < len(bpe_tokens) and cur_len < target:\n                tok = bpe_tokens[j]\n                # remove end-of-word marker if present\n                t = tok.replace('</w>', '')\n                seq.append(tok)\n                cur_len += len(t)\n                j += 1\n            if seq:\n                consistency_map[s][tuple(seq)] += 1\n\n        # occasional checkpoint printing\n        if (i + 1) % progress_interval == 0:\n            print(f\"-- processed {i+1}/{len(eval_df)} rows\")\n\n    resdf = pd.DataFrame(results)\n    # corpus-level metrics (mean/median and consistency)\n    summary = {\n        'rows_evaluated': len(resdf),\n        'vocab_jaccard_mean': resdf['vocab_jaccard'].mean(),\n        'vocab_jaccard_med': resdf['vocab_jaccard'].median(),\n        'compression_ratio_mean': resdf['compression_ratio'].mean(),\n        'boundary_f1_mean': resdf['boundary_f1'].mean(),\n        'oov_rate_mean': resdf['oov_rate'].mean()\n    }\n    consistency_score = compute_consistency_score(consistency_map)\n\n    # save detail and summary\n    resdf.to_csv(save_path, index=False)\n    meta = {'summary': summary, 'consistency_score': consistency_score, 'sample': sample_n}\n    with open(save_path.replace('.csv', '.meta.json'), 'w', encoding='utf-8') as f:\n        json.dump(meta, f, indent=2)\n\n    print(\"Saved per-row results to:\", save_path)\n    print(\"Summary:\", summary)\n    print(\"Consistency score (weighted):\", consistency_score)\n    return resdf, summary, consistency_score, consistency_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:37:32.862305Z","iopub.execute_input":"2025-09-20T13:37:32.862591Z","iopub.status.idle":"2025-09-20T13:37:32.875330Z","shell.execute_reply.started":"2025-09-20T13:37:32.862571Z","shell.execute_reply":"2025-09-20T13:37:32.874341Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# cell 6: additional global stats (OOVs and vocab overlap)\ndef global_vocab_and_oov_stats(bpe: ResumableBPE, df: pd.DataFrame, col_tokens: str):\n    # build ground-truth vocab\n    gt_vocab = Counter()\n    for tokens in df[col_tokens].fillna('').tolist():\n        for t in ensure_list(tokens):\n            gt_vocab[t] += 1\n\n    bpe_vocab = set(bpe.vocab)\n    gt_vocab_set = set(gt_vocab.keys())\n\n    jacc = jaccard(gt_vocab_set, bpe_vocab)\n    oovs = [t for t in gt_vocab_set if t not in bpe_vocab]\n    oov_counts = sum(gt_vocab[t] for t in oovs)\n    total_tokens = sum(gt_vocab.values())\n    oov_rate = oov_counts / total_tokens if total_tokens else 0.0\n\n    return {\n        'gt_vocab_size': len(gt_vocab_set),\n        'bpe_vocab_size': len(bpe_vocab),\n        'vocab_jaccard_global': jacc,\n        'oov_token_count': len(oovs),\n        'oov_token_types_sample': oovs[:50],\n        'oov_token_total_occurrences': oov_counts,\n        'oov_rate_by_token_occurrence': oov_rate\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:37:51.764629Z","iopub.execute_input":"2025-09-20T13:37:51.765295Z","iopub.status.idle":"2025-09-20T13:37:51.772071Z","shell.execute_reply.started":"2025-09-20T13:37:51.765267Z","shell.execute_reply":"2025-09-20T13:37:51.771155Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# cell 7: run evaluation example (adjust paths/prefixes)\n# 1) load df if not loaded already\n# df = pd.read_pickle('your_dataframe.pkl')  # or read_csv\n\n# 2) load bpe\nbpe_prefix = 'bpe_code_only_tokenizer_dev'   # change for docs-only or combined\nbpe = load_bpe_from_prefix(bpe_prefix)\n\n# 3) run corpus evaluation (sample if dataset large)\n# Evaluate code tokens\nres_code, summary_code, consistency_code, cons_map_code = evaluate_corpus(\n    bpe=bpe,\n    df=df,\n    col_tokens='code_tokens',\n    col_text='code',         # optional: original string column; better encoding if present\n    sample_n=5000,           # set to None to run full dataset (careful on big sets)\n    save_path=f'bpe_eval_{bpe_prefix}_code.csv',\n    progress_interval=500\n)\n\n# Evaluate docstring tokens\nres_docs, summary_docs, consistency_docs, cons_map_docs = evaluate_corpus(\n    bpe=bpe,\n    df=df,\n    col_tokens='docstring_tokens',\n    col_text='docstring',\n    sample_n=5000,\n    save_path=f'bpe_eval_{bpe_prefix}_docs.csv',\n    progress_interval=500\n)\n\n# 4) global vocab/OOV stats\nglobal_code_stats = global_vocab_and_oov_stats(bpe, df, 'code_tokens')\nglobal_doc_stats = global_vocab_and_oov_stats(bpe, df, 'docstring_tokens')\n\nprint(\"Global code stats:\", global_code_stats)\nprint(\"Global docstring stats:\", global_doc_stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:38:07.193332Z","iopub.execute_input":"2025-09-20T13:38:07.193948Z","iopub.status.idle":"2025-09-20T13:40:28.858156Z","shell.execute_reply.started":"2025-09-20T13:38:07.193911Z","shell.execute_reply":"2025-09-20T13:40:28.857174Z"}},"outputs":[{"name":"stdout","text":"Loaded tokenizer from bpe_code_only_tokenizer_dev. Vocab size: 32000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating code_tokens:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c029e44ff94895aa901a0e4f13a027"}},"metadata":{}},{"name":"stdout","text":"-- processed 500/5000 rows\n-- processed 1000/5000 rows\n-- processed 1500/5000 rows\n-- processed 2000/5000 rows\n-- processed 2500/5000 rows\n-- processed 3000/5000 rows\n-- processed 3500/5000 rows\n-- processed 4000/5000 rows\n-- processed 4500/5000 rows\n-- processed 5000/5000 rows\nSaved per-row results to: bpe_eval_bpe_code_only_tokenizer_dev_code.csv\nSummary: {'rows_evaluated': 5000, 'vocab_jaccard_mean': 0.004949415381898779, 'vocab_jaccard_med': 0.0, 'compression_ratio_mean': 1.5829690592830488, 'boundary_f1_mean': 0.15457856968084624, 'oov_rate_mean': 0.10102402568584905}\nConsistency score (weighted): 0.10621478199026205\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating docstring_tokens:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57d79023a5434c28ba991513d0cceb45"}},"metadata":{}},{"name":"stdout","text":"-- processed 500/5000 rows\n-- processed 1000/5000 rows\n-- processed 1500/5000 rows\n-- processed 2000/5000 rows\n-- processed 2500/5000 rows\n-- processed 3000/5000 rows\n-- processed 3500/5000 rows\n-- processed 4000/5000 rows\n-- processed 4500/5000 rows\n-- processed 5000/5000 rows\nSaved per-row results to: bpe_eval_bpe_code_only_tokenizer_dev_docs.csv\nSummary: {'rows_evaluated': 5000, 'vocab_jaccard_mean': 0.0015589743728958436, 'vocab_jaccard_med': 0.0, 'compression_ratio_mean': 0.5227851328419996, 'boundary_f1_mean': 0.07415898803186012, 'oov_rate_mean': 0.31857325068747894}\nConsistency score (weighted): 0.9354862207114241\nGlobal code stats: {'gt_vocab_size': 1343891, 'bpe_vocab_size': 32000, 'vocab_jaccard_global': 0.008181868141933356, 'oov_token_count': 1332725, 'oov_token_types_sample': ['', 'build_component', 'calendar_events', '#   * Qualifiers contain NOT', 'sxv', 'mag_idx', 'tunein', '# if we hit a new state', 'first_lt', '_update_ranges', 'ldrtl', 'sentenel', 'move_datetime_month', 'gene2pubmed', 'return_med', 'squared_phase', 'hprinted', 'tax_min', 'sInput', 'primeSieve', 'vec_n', 'jsonRpcLogout', '_build_field_type_map', 'zap_disk', 'trimvariables', 'x_no_none', 'whitelisted_barcode', '23456789BCDFGHJKMNPQRTVWXY', 'passer_rating_index', 'ApplyOnly', 'compact_firmware_file', 'Amazon', 'sciobj_pid_list', '# been written in a round-robin fashion. This assumption does not hold, for', '# Start observe loop', '_vertex_class_names', 'JLDCorpus', 'BUILD_LIST_UNPACK', 'EC2_INSTANCE_TYPES', 'googlesearchid', 'kde_home', 'calculate_split_from_extents', 'AppCompatCache', 'h_shape', 'pyrax', 'ops_filter', 'delete_bandwidth_group', 'renderers', 'nThen', 'writeObject'], 'oov_token_total_occurrences': 13269773, 'oov_rate_by_token_occurrence': 0.0651861945814471}\nGlobal docstring stats: {'gt_vocab_size': 188024, 'bpe_vocab_size': 32000, 'vocab_jaccard_global': 0.03127709736537443, 'oov_token_count': 181351, 'oov_token_types_sample': ['11011', 'Conform', 'temp_dir', 'CWRFile', 'awslimitchecker', 'DSDL', 'XUngrabKeyboard', 'Successive', 'Zotero', 'MotorInfo', 'gene2pubmed', 'arounds', 'display_matches', 'show_headers', 'BookstoreSettings', 'tax_min', '获取单个歌曲', 'get_config_setting', 'True?', 'SNMPv3', 'Tnfrsf4', 'Amazon', 'setAllToDefaults', 'содержать', 'ForPath', 'stacker', 'CFG_WEBCOMMENT_TIMELIMIT_PROCESSING_COMMENTS_IN_SECONDS', '__metaclass__', 'K线类型', 'logadm', 'with_cloud_account', 'download_metadata<ohapi', '40069325', 'repliers', 'Collected', 'getDrivingDistance2D', '22237709', 'BaseMethod', '1776', 'deben', 'Chisel', 'mans', 'QUADRATIC', 'owlsim', 'ParameterGroup', 'MGMT', 'Находит', '_set_peer', 'IndexAPIProcessor', 'bugreports'], 'oov_token_total_occurrences': 2224642, 'oov_rate_by_token_occurrence': 0.29808615312382925}\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## second evaluation ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import jaccard_score\nfrom collections import Counter\n\n# load your dataset if not already loaded\n# df = pd.read_parquet(\"your_dataset.parquet\")\n\n# select last 17k rows for evaluation\neval_df = df.tail(17000).reset_index(drop=True)\nprint(eval_df.shape)\n\n# Load your trained tokenizer (pick code/docs/combined)\nbpe = ResumableBPE()\nbpe.load(\"bpe_code_only_tokenizer_dev\")  # or docs_only / combined\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:54:35.766659Z","iopub.execute_input":"2025-09-20T13:54:35.767218Z","iopub.status.idle":"2025-09-20T13:54:45.854661Z","shell.execute_reply.started":"2025-09-20T13:54:35.767185Z","shell.execute_reply":"2025-09-20T13:54:45.853567Z"}},"outputs":[{"name":"stdout","text":"(17000, 13)\nLoaded tokenizer from bpe_code_only_tokenizer_dev. Vocab size: 32000\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"def jaccard_vocab_overlap(bpe, ground_truth_tokens):\n    # flatten gt vocab\n    gt_vocab = set([tok for toks in ground_truth_tokens for tok in toks])\n    bpe_vocab = set(bpe.vocab)\n    intersection = len(gt_vocab & bpe_vocab)\n    union = len(gt_vocab | bpe_vocab)\n    return intersection / union if union else 0\n\ndef compression_ratio(bpe, texts, ground_truth_tokens):\n    ratios = []\n    for text, gt_tokens in zip(texts, ground_truth_tokens):\n        bpe_ids = bpe.encode(text)\n        ratios.append(len(gt_tokens) / max(1, len(bpe_ids)))\n    return np.mean(ratios)\n\ndef boundary_accuracy(bpe, texts, ground_truth_tokens):\n    # how often bpe token boundaries align with gt tokens\n    correct = total = 0\n    for text, gt_tokens in zip(texts, ground_truth_tokens):\n        # reconstruct word boundaries by concatenation\n        bpe_tokens = [bpe.id_to_token[i] for i in bpe.encode(text)]\n        # crude: check if gt tokens appear as contiguous subsequences\n        joined = ''.join(bpe_tokens).replace('</w>', ' ')\n        pred_tokens = joined.split()\n        correct += sum(1 for a, b in zip(pred_tokens, gt_tokens) if a == b)\n        total += len(gt_tokens)\n    return correct / total if total else 0\n\ndef consistency_score(bpe, texts):\n    # same input string should always tokenize identically\n    scores = []\n    for text in texts[:500]:  # sample to save time\n        t1 = bpe.encode(text)\n        t2 = bpe.encode(text)\n        scores.append(int(t1 == t2))\n    return np.mean(scores)\n\ndef oov_rate(bpe, ground_truth_tokens):\n    gt_vocab = set([tok for toks in ground_truth_tokens for tok in toks])\n    oov = [tok for tok in gt_vocab if tok not in bpe.token_to_id]\n    return len(oov) / len(gt_vocab) if gt_vocab else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:59:43.477519Z","iopub.execute_input":"2025-09-20T13:59:43.477865Z","iopub.status.idle":"2025-09-20T13:59:43.489187Z","shell.execute_reply.started":"2025-09-20T13:59:43.477838Z","shell.execute_reply":"2025-09-20T13:59:43.488277Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"code_texts = eval_df[\"code\"].tolist()\ncode_gt = eval_df[\"code_tokens\"].tolist()\n\nprint(\"📊 CODE TOKENIZER EVALUATION\")\nprint(\"Vocab overlap (Jaccard):\", jaccard_vocab_overlap(bpe, code_gt))\nprint(\"Compression ratio:\", compression_ratio(bpe, code_texts, code_gt))\nprint(\"Boundary accuracy:\", boundary_accuracy(bpe, code_texts, code_gt))\nprint(\"Consistency:\", consistency_score(bpe, code_texts))\nprint(\"OOV rate:\", oov_rate(bpe, code_gt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:59:57.345641Z","iopub.execute_input":"2025-09-20T13:59:57.345977Z","iopub.status.idle":"2025-09-20T14:01:36.803158Z","shell.execute_reply.started":"2025-09-20T13:59:57.345954Z","shell.execute_reply":"2025-09-20T14:01:36.802125Z"}},"outputs":[{"name":"stdout","text":"📊 CODE TOKENIZER EVALUATION\nVocab overlap (Jaccard): 0.013412584801145204\nCompression ratio: 3.775157678802337\nBoundary accuracy: 0.004530778414079158\nConsistency: 1.0\nOOV rate: 0.23716814159292035\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"docs_texts = eval_df[\"docstring\"].tolist()\ndocs_gt = eval_df[\"docstring_tokens\"].tolist()\n\nprint(\"📊 DOCSTRING TOKENIZER EVALUATION\")\nprint(\"Vocab overlap (Jaccard):\", jaccard_vocab_overlap(bpe, docs_gt))\nprint(\"Compression ratio:\", compression_ratio(bpe, docs_texts, docs_gt))\nprint(\"Boundary accuracy:\", boundary_accuracy(bpe, docs_texts, docs_gt))\nprint(\"Consistency:\", consistency_score(bpe, docs_texts))\nprint(\"OOV rate:\", oov_rate(bpe, docs_gt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T14:01:51.728003Z","iopub.execute_input":"2025-09-20T14:01:51.728352Z","iopub.status.idle":"2025-09-20T14:02:21.046558Z","shell.execute_reply.started":"2025-09-20T14:01:51.728329Z","shell.execute_reply":"2025-09-20T14:02:21.045579Z"}},"outputs":[{"name":"stdout","text":"📊 DOCSTRING TOKENIZER EVALUATION\nVocab overlap (Jaccard): 0.020904078008660707\nCompression ratio: 4.281768495781281\nBoundary accuracy: 0.0033190137518516885\nConsistency: 1.0\nOOV rate: 0.12857142857142856\n","output_type":"stream"}],"execution_count":71}]}